#!/usr/bin/env node

/**
 * Comprehensive Edge Functions Testing Suite
 * Tests migrated Supabase Edge Functions for API compatibility and functionality
 */

const fetch = require('node-fetch');
const crypto = require('crypto');

// Configuration
const SUPABASE_URL = 'https://kvsqtolsjggpyvdtdpss.supabase.co';
const TEST_CONFIG = {
  // Test endpoints
  UPLOAD_URL: `${SUPABASE_URL}/functions/v1/upload-document-to-openai`,
  ASSISTANT_URL: `${SUPABASE_URL}/functions/v1/openai-assistant`,
  
  // Test data
  TEST_USER_TOKEN: process.env.SUPABASE_TEST_USER_TOKEN,
  TEST_VECTOR_STORE_ID: process.env.TEST_VECTOR_STORE_ID,
  
  // Test parameters
  TIMEOUT: 30000, // 30 seconds
  RETRY_COUNT: 3,
  CONCURRENT_REQUESTS: 5
};\n\n// Colors for console output\nconst colors = {\n  reset: '\\x1b[0m',\n  bright: '\\x1b[1m',\n  red: '\\x1b[31m',\n  green: '\\x1b[32m',\n  yellow: '\\x1b[33m',\n  blue: '\\x1b[34m',\n  magenta: '\\x1b[35m',\n  cyan: '\\x1b[36m'\n};\n\n// Test result tracking\nconst testResults = {\n  passed: 0,\n  failed: 0,\n  errors: [],\n  performance: []\n};\n\n// Utility functions\nfunction log(message, color = 'reset') {\n  console.log(`${colors[color]}${message}${colors.reset}`);\n}\n\nfunction logSuccess(message) {\n  log(`✅ ${message}`, 'green');\n  testResults.passed++;\n}\n\nfunction logError(message, error = null) {\n  log(`❌ ${message}`, 'red');\n  if (error) {\n    log(`   Error: ${error.message}`, 'red');\n    testResults.errors.push({ message, error: error.message });\n  }\n  testResults.failed++;\n}\n\nfunction logWarning(message) {\n  log(`⚠️ ${message}`, 'yellow');\n}\n\nfunction logInfo(message) {\n  log(`ℹ️ ${message}`, 'blue');\n}\n\n// Performance tracking\nfunction trackPerformance(testName, duration, success) {\n  testResults.performance.push({\n    test: testName,\n    duration,\n    success,\n    timestamp: new Date().toISOString()\n  });\n}\n\n// HTTP request wrapper with error handling\nasync function makeRequest(url, options, testName) {\n  const startTime = Date.now();\n  \n  try {\n    const controller = new AbortController();\n    const timeoutId = setTimeout(() => controller.abort(), TEST_CONFIG.TIMEOUT);\n    \n    const response = await fetch(url, {\n      ...options,\n      signal: controller.signal\n    });\n    \n    clearTimeout(timeoutId);\n    const duration = Date.now() - startTime;\n    \n    // Track performance\n    trackPerformance(testName, duration, response.ok);\n    \n    if (duration > 10000) {\n      logWarning(`${testName} took ${duration}ms (>10s)`);\n    }\n    \n    return { response, duration };\n  } catch (error) {\n    const duration = Date.now() - startTime;\n    trackPerformance(testName, duration, false);\n    throw error;\n  }\n}\n\n// Test Suite Classes\nclass SecurityTests {\n  static async runAll() {\n    log('\\n🔒 Running Security Tests', 'cyan');\n    \n    await this.testCorsValidation();\n    await this.testAuthenticationRequired();\n    await this.testInvalidTokens();\n    await this.testRateLimiting();\n    await this.testInputValidation();\n  }\n  \n  static async testCorsValidation() {\n    logInfo('Testing CORS validation...');\n    \n    try {\n      // Test with unauthorized origin\n      const { response } = await makeRequest(TEST_CONFIG.UPLOAD_URL, {\n        method: 'POST',\n        headers: {\n          'Origin': 'https://malicious-site.com',\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({ test: 'data' })\n      }, 'CORS Validation');\n      \n      if (response.status === 403) {\n        logSuccess('CORS properly blocks unauthorized origins');\n      } else {\n        logError('CORS validation failed - unauthorized origin allowed');\n      }\n    } catch (error) {\n      logError('CORS test failed', error);\n    }\n  }\n  \n  static async testAuthenticationRequired() {\n    logInfo('Testing authentication requirements...');\n    \n    try {\n      // Test without authorization header\n      const { response } = await makeRequest(TEST_CONFIG.UPLOAD_URL, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          supabaseFilePath: 'test.pdf',\n          vectorStoreId: 'test',\n          title: 'Test'\n        })\n      }, 'Auth Required Test');\n      \n      if (response.status === 401) {\n        logSuccess('Authentication properly required');\n      } else {\n        logError('Authentication validation failed');\n      }\n    } catch (error) {\n      logError('Authentication test failed', error);\n    }\n  }\n  \n  static async testInvalidTokens() {\n    logInfo('Testing invalid token handling...');\n    \n    try {\n      const { response } = await makeRequest(TEST_CONFIG.UPLOAD_URL, {\n        method: 'POST',\n        headers: {\n          'Authorization': 'Bearer invalid-token-12345',\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          supabaseFilePath: 'test.pdf',\n          vectorStoreId: 'test',\n          title: 'Test'\n        })\n      }, 'Invalid Token Test');\n      \n      if (response.status === 401) {\n        logSuccess('Invalid tokens properly rejected');\n      } else {\n        logError('Invalid token validation failed');\n      }\n    } catch (error) {\n      logError('Invalid token test failed', error);\n    }\n  }\n  \n  static async testRateLimiting() {\n    logInfo('Testing rate limiting...');\n    \n    if (!TEST_CONFIG.TEST_USER_TOKEN) {\n      logWarning('Skipping rate limiting test - no test token provided');\n      return;\n    }\n    \n    try {\n      // Make multiple rapid requests\n      const requests = Array(15).fill().map((_, i) => \n        makeRequest(TEST_CONFIG.UPLOAD_URL, {\n          method: 'POST',\n          headers: {\n            'Authorization': `Bearer ${TEST_CONFIG.TEST_USER_TOKEN}`,\n            'Content-Type': 'application/json'\n          },\n          body: JSON.stringify({\n            supabaseFilePath: `test${i}.pdf`,\n            vectorStoreId: 'test',\n            title: `Test ${i}`,\n            fileSize: 1000000 // 1MB\n          })\n        }, `Rate Limit Test ${i}`)\n      );\n      \n      const results = await Promise.allSettled(requests);\n      const rateLimited = results.some(result => \n        result.status === 'fulfilled' && result.value.response.status === 429\n      );\n      \n      if (rateLimited) {\n        logSuccess('Rate limiting properly enforced');\n      } else {\n        logWarning('Rate limiting may not be working - no 429 responses');\n      }\n    } catch (error) {\n      logError('Rate limiting test failed', error);\n    }\n  }\n  \n  static async testInputValidation() {\n    logInfo('Testing input validation...');\n    \n    if (!TEST_CONFIG.TEST_USER_TOKEN) {\n      logWarning('Skipping input validation test - no test token provided');\n      return;\n    }\n    \n    try {\n      // Test with missing required fields\n      const { response } = await makeRequest(TEST_CONFIG.UPLOAD_URL, {\n        method: 'POST',\n        headers: {\n          'Authorization': `Bearer ${TEST_CONFIG.TEST_USER_TOKEN}`,\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          // Missing required fields\n          title: 'Test'\n        })\n      }, 'Input Validation Test');\n      \n      if (response.status === 400) {\n        logSuccess('Input validation properly enforced');\n      } else {\n        logError('Input validation failed');\n      }\n    } catch (error) {\n      logError('Input validation test failed', error);\n    }\n  }\n}\n\nclass FunctionalityTests {\n  static async runAll() {\n    log('\\n⚡ Running Functionality Tests', 'cyan');\n    \n    await this.testOptionsMethod();\n    await this.testAssistantBasicFlow();\n    await this.testAssistantMessageRetrieval();\n    await this.testErrorHandling();\n  }\n  \n  static async testOptionsMethod() {\n    logInfo('Testing CORS preflight (OPTIONS method)...');\n    \n    try {\n      const { response } = await makeRequest(TEST_CONFIG.UPLOAD_URL, {\n        method: 'OPTIONS',\n        headers: {\n          'Origin': 'http://localhost:5173',\n          'Access-Control-Request-Method': 'POST',\n          'Access-Control-Request-Headers': 'Content-Type, Authorization'\n        }\n      }, 'OPTIONS Method Test');\n      \n      if (response.ok && response.headers.get('access-control-allow-origin')) {\n        logSuccess('OPTIONS method properly handled');\n      } else {\n        logError('OPTIONS method handling failed');\n      }\n    } catch (error) {\n      logError('OPTIONS method test failed', error);\n    }\n  }\n  \n  static async testAssistantBasicFlow() {\n    logInfo('Testing assistant basic flow...');\n    \n    if (!TEST_CONFIG.TEST_USER_TOKEN) {\n      logWarning('Skipping assistant test - no test token provided');\n      return;\n    }\n    \n    try {\n      const { response, duration } = await makeRequest(TEST_CONFIG.ASSISTANT_URL, {\n        method: 'POST',\n        headers: {\n          'Authorization': `Bearer ${TEST_CONFIG.TEST_USER_TOKEN}`,\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          message: 'Hello, this is a test message for migration validation.',\n          conversationId: `test-conversation-${Date.now()}`\n        })\n      }, 'Assistant Basic Flow');\n      \n      if (response.ok) {\n        const data = await response.json();\n        if (data.message && data.metadata) {\n          logSuccess(`Assistant flow working (${duration}ms)`);\n        } else {\n          logError('Assistant response missing required fields');\n        }\n      } else {\n        const errorText = await response.text();\n        logError(`Assistant request failed: ${response.status} - ${errorText}`);\n      }\n    } catch (error) {\n      logError('Assistant basic flow test failed', error);\n    }\n  }\n  \n  static async testAssistantMessageRetrieval() {\n    logInfo('Testing assistant message retrieval...');\n    \n    if (!TEST_CONFIG.TEST_USER_TOKEN) {\n      logWarning('Skipping message retrieval test - no test token provided');\n      return;\n    }\n    \n    try {\n      const { response } = await makeRequest(TEST_CONFIG.ASSISTANT_URL, {\n        method: 'POST',\n        headers: {\n          'Authorization': `Bearer ${TEST_CONFIG.TEST_USER_TOKEN}`,\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          message: '__GET_MESSAGES__',\n          threadId: 'thread_test123'\n        })\n      }, 'Message Retrieval Test');\n      \n      if (response.ok) {\n        const data = await response.json();\n        if (data.messages !== undefined && data.metadata) {\n          logSuccess('Message retrieval working');\n        } else {\n          logError('Message retrieval response malformed');\n        }\n      } else {\n        logError(`Message retrieval failed: ${response.status}`);\n      }\n    } catch (error) {\n      logError('Message retrieval test failed', error);\n    }\n  }\n  \n  static async testErrorHandling() {\n    logInfo('Testing error handling...');\n    \n    try {\n      // Test with malformed JSON\n      const { response } = await makeRequest(TEST_CONFIG.ASSISTANT_URL, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: 'invalid-json{'\n      }, 'Error Handling Test');\n      \n      if (response.status >= 400 && response.status < 500) {\n        logSuccess('Error handling working properly');\n      } else {\n        logError('Error handling failed');\n      }\n    } catch (error) {\n      // This is expected for malformed requests\n      logSuccess('Error handling working (network error expected)');\n    }\n  }\n}\n\nclass PerformanceTests {\n  static async runAll() {\n    log('\\n🚀 Running Performance Tests', 'cyan');\n    \n    await this.testResponseTimes();\n    await this.testConcurrentRequests();\n    await this.testLargePayloads();\n  }\n  \n  static async testResponseTimes() {\n    logInfo('Testing response times...');\n    \n    if (!TEST_CONFIG.TEST_USER_TOKEN) {\n      logWarning('Skipping response time test - no test token provided');\n      return;\n    }\n    \n    try {\n      const { duration } = await makeRequest(TEST_CONFIG.ASSISTANT_URL, {\n        method: 'POST',\n        headers: {\n          'Authorization': `Bearer ${TEST_CONFIG.TEST_USER_TOKEN}`,\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          message: 'Quick test message',\n          conversationId: `perf-test-${Date.now()}`\n        })\n      }, 'Response Time Test');\n      \n      if (duration < 5000) {\n        logSuccess(`Response time acceptable: ${duration}ms`);\n      } else if (duration < 10000) {\n        logWarning(`Response time slow: ${duration}ms`);\n      } else {\n        logError(`Response time too slow: ${duration}ms`);\n      }\n    } catch (error) {\n      logError('Response time test failed', error);\n    }\n  }\n  \n  static async testConcurrentRequests() {\n    logInfo('Testing concurrent request handling...');\n    \n    if (!TEST_CONFIG.TEST_USER_TOKEN) {\n      logWarning('Skipping concurrent test - no test token provided');\n      return;\n    }\n    \n    try {\n      const requests = Array(TEST_CONFIG.CONCURRENT_REQUESTS).fill().map((_, i) =>\n        makeRequest(TEST_CONFIG.ASSISTANT_URL, {\n          method: 'POST',\n          headers: {\n            'Authorization': `Bearer ${TEST_CONFIG.TEST_USER_TOKEN}`,\n            'Content-Type': 'application/json'\n          },\n          body: JSON.stringify({\n            message: `Concurrent test message ${i}`,\n            conversationId: `concurrent-test-${i}-${Date.now()}`\n          })\n        }, `Concurrent Test ${i}`)\n      );\n      \n      const startTime = Date.now();\n      const results = await Promise.allSettled(requests);\n      const totalTime = Date.now() - startTime;\n      \n      const successful = results.filter(r => \n        r.status === 'fulfilled' && r.value.response.ok\n      ).length;\n      \n      logInfo(`Concurrent requests: ${successful}/${TEST_CONFIG.CONCURRENT_REQUESTS} successful in ${totalTime}ms`);\n      \n      if (successful >= TEST_CONFIG.CONCURRENT_REQUESTS * 0.8) {\n        logSuccess('Concurrent request handling acceptable');\n      } else {\n        logError('Concurrent request handling failed');\n      }\n    } catch (error) {\n      logError('Concurrent request test failed', error);\n    }\n  }\n  \n  static async testLargePayloads() {\n    logInfo('Testing large payload handling...');\n    \n    if (!TEST_CONFIG.TEST_USER_TOKEN) {\n      logWarning('Skipping large payload test - no test token provided');\n      return;\n    }\n    \n    try {\n      // Create a large message\n      const largeMessage = 'A'.repeat(10000); // 10KB message\n      \n      const { response, duration } = await makeRequest(TEST_CONFIG.ASSISTANT_URL, {\n        method: 'POST',\n        headers: {\n          'Authorization': `Bearer ${TEST_CONFIG.TEST_USER_TOKEN}`,\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          message: largeMessage,\n          conversationId: `large-payload-test-${Date.now()}`\n        })\n      }, 'Large Payload Test');\n      \n      if (response.ok) {\n        logSuccess(`Large payload handled successfully (${duration}ms)`);\n      } else {\n        logError(`Large payload failed: ${response.status}`);\n      }\n    } catch (error) {\n      logError('Large payload test failed', error);\n    }\n  }\n}\n\n// Test report generation\nfunction generateReport() {\n  log('\\n📊 Test Results Summary', 'magenta');\n  log('=' .repeat(50), 'magenta');\n  \n  const total = testResults.passed + testResults.failed;\n  const successRate = total > 0 ? (testResults.passed / total * 100).toFixed(1) : 0;\n  \n  log(`Total Tests: ${total}`);\n  log(`Passed: ${testResults.passed}`, 'green');\n  log(`Failed: ${testResults.failed}`, testResults.failed > 0 ? 'red' : 'reset');\n  log(`Success Rate: ${successRate}%`, successRate >= 90 ? 'green' : 'yellow');\n  \n  if (testResults.performance.length > 0) {\n    const avgDuration = testResults.performance.reduce((acc, p) => acc + p.duration, 0) / testResults.performance.length;\n    const maxDuration = Math.max(...testResults.performance.map(p => p.duration));\n    \n    log(`\\nPerformance Metrics:`);\n    log(`Average Response Time: ${avgDuration.toFixed(0)}ms`);\n    log(`Max Response Time: ${maxDuration}ms`);\n  }\n  \n  if (testResults.errors.length > 0) {\n    log('\\nErrors:', 'red');\n    testResults.errors.forEach(error => {\n      log(`  • ${error.message}: ${error.error}`, 'red');\n    });\n  }\n  \n  // Migration readiness assessment\n  log('\\n🎯 Migration Readiness Assessment', 'cyan');\n  if (successRate >= 95 && testResults.failed === 0) {\n    log('✅ READY FOR MIGRATION - All tests passed', 'green');\n  } else if (successRate >= 90) {\n    log('⚠️ READY WITH CAUTION - Minor issues detected', 'yellow');\n  } else {\n    log('❌ NOT READY - Critical issues detected', 'red');\n  }\n}\n\n// Main execution\nasync function runAllTests() {\n  log('🧪 Netlify to Supabase Migration - Edge Functions Test Suite', 'bright');\n  log('================================================================', 'bright');\n  \n  // Validate configuration\n  if (!TEST_CONFIG.TEST_USER_TOKEN) {\n    logWarning('SUPABASE_TEST_USER_TOKEN not provided - some tests will be skipped');\n    logInfo('To run full tests, set: export SUPABASE_TEST_USER_TOKEN=\"your-token\"');\n  }\n  \n  if (!TEST_CONFIG.TEST_VECTOR_STORE_ID) {\n    logWarning('TEST_VECTOR_STORE_ID not provided - upload tests will be limited');\n  }\n  \n  try {\n    // Run test suites\n    await SecurityTests.runAll();\n    await FunctionalityTests.runAll();\n    await PerformanceTests.runAll();\n    \n    // Generate final report\n    generateReport();\n    \n  } catch (error) {\n    logError('Test suite execution failed', error);\n    process.exit(1);\n  }\n}\n\n// Export for use as module\nif (require.main === module) {\n  runAllTests().catch(error => {\n    console.error('Fatal error:', error);\n    process.exit(1);\n  });\n}\n\nmodule.exports = {\n  runAllTests,\n  SecurityTests,\n  FunctionalityTests,\n  PerformanceTests,\n  testResults\n};